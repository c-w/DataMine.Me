Date	URL	Title	Topic	Summary
2014-08-11	http://web.stanford.edu/~jpennin/papers/glove.pdf	GloVe: Global Vectors for Word Representation	Natural Language Processing	Semantic modelling: combine word-co-occurrence approach with word-context approach to get significantly better performance. Only consider non-zero elements of co-occurrence matrix in order to reduce noise. Not all training data is created equal: less in-domain data can outperform more out-of-domain data.
2014-08-13	http://research.microsoft.com/en-us/um/people/nickcr/wscd2014/papers/wscdchallenge2014dataiku.pdf	Dataiku’s Solution to Yandex’s Personalized Web Search Challenge	Data Science	Winning a Kaggle learning-to-rank challenge. Interesting parts: using collaborative filtering for search re-ranking, quality of search snippet as a feature, trying to break the class-labels in the dataset up further. Hyper-parameter tuning was important.
2014-08-15	http://arxiv.org/pdf/1207.4169.pdf	The Author-Topic Model for Authors and Documents	Natural Language Processing	Augmenting LDA topic modelling with author information: opens up applications in stylometry and recommendations.
2014-08-17	http://www.win.tue.nl/~laroyo/2L340/resources/Amazon-Recommendations.pdf	Item-to-Item Collaborative Filtering	Machine Learning	Collaborative filtering finds similar users (e.g. using cosine similarity) and recommends items that one likes but not the other. Slow: O(M+N) where M is the number of users and N is the number of items. Can speed it up using dimensionality reduction but that hurts performance. Cluster-based recommendations split the user population into categories and generate local recommendations within the categories. Faster than collaborative filtering but still slow. Usually poor performance because clustering is not granular enough. Search-based recommendations use past interests as search queries to generate recommendations. Performs poorly for customers with large purchase histories due to difficulty of selecting the relevant items to based the query on. Item-to-item collaborative filtering finds similar items to the ones that a user likes and recommends those (e.g. by looking at items that are frequently purchased together - very sparse so easy to compute, can be computed off-line; on-line complexity is only related to user purchase history so essentially constant time).
2014-08-17	http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf	Top 10 algorithms in data mining	Machine Learning	Gentle overview of some basic machine learning algorithms. Some interesting observations. K-means: can fit non-spherical clusters by using more sophisticated distance metrics (e.g. KL divergence) or we can run k-means with large k and then use agglomerative clustering as a post-processing step (this technique also means that we don't have to select k and aren't sensitive to initial centroid choice). Apriori algorithm for frequent-item-set inference: iteratively grow and prune item-sets to avoid combinatorial explosion. Ada boost algorithm: iteratively fit a weak algorithm to a data-set and re-weight the data by the error of the learner (i.e. in the next round we'll learn a model that prioritizes the errors of the previous one), then get overall prediction using weighted average of all learners: ensemble learning technique outperforms performance of the individual learners. K-NN: it's possible to remove most of the data-points without harming accuracy. Naive Bayes: get a non-linear decision boundary by including interaction terms between attributes.
2014-08-18	http://arxiv.org/pdf/1404.3056.pdf	Principles of Antifragile Software	Software Engineering	Antifragility: getting better in the presence of errors. Examples: code that patches itself when encountering a bug. E.g. Netflix randomly crashes servers or increases/decreases latency to test their distributed algorithms. Need to minimize impact of errors e.g. via TDD + continuous deployment.
2014-08-22	http://arxiv.org/pdf/1408.3218v1.pdf	Toward Automated Discovery of Artistic Influence	Computer Vision	Task: automatically finding all the influences on some painter. Unsupervised knowledge discovery task. Difficult. Solve correlated supervised task instead: style classification. Unsurprisingly, custom made high-level semantic features work better than generic low level features like SIFT.
2014-08-26	http://research.microsoft.com/pubs/167719/WhyFromNigeria.pdf	Why do Nigerian Scammers Say They are from Nigeria?	Machine Learning	Using reasoning based on ROC curves to find analytic bounds of classifier performance.
2014-08-26	http://delivery.acm.org/10.1145/1730000/1722966/p4-su.pdf	A Survey of Collaborative Filtering Techniques	Machine Learning	Main challenges in recommenders. Data sparsity: use dimensionality reduction (LSI, PCA) or try to do local recommendations in some cluster e.g. by using a decision tree or explicit clustering or external information (like product taxonomy). Scalability: there exists an incremental version of SVD,  if possible only compare co-rated items. Synonymy: LSI fixed this automatically or domain specific thesaurus. Shilling attacks: item-based more robust, explicitly account for it during data normalization, work with similarity residuals instead of absolute values. Memory-based collaborative filtering algorithm: find similar users (e.g. Pearson correlation or cosine similarity) and then aggregate their likes into recommendations (e.g. using voting or weighted averages). Model-based algorithm: Bayes net or clustering. Hybrids incorporating domain knowledge or extra structure (e.g. demographics) work best. Evaluation: MAE or RMSE or ROC AUC.
2014-08-26	http://wwwold.cs.umd.edu/~elaine/docs/scambaiter.pdf	Scambaiter: Understanding Targeted Nigerian Scams on Craigslist	Data Science	Setting up a honey-pot to collect data about 491 scams. IP address and reply-to email address that differs from sent-from address are highly indicative features of scams. Images in emails that include images hosted on external servers are uncannily effective at gathering personal information via emails.
2014-08-29	http://2013.berlinbuzzwords.de/sites/2013.berlinbuzzwords.de/files/slides/DanFilimon.pdf	Clustering data at scale	Machine Learning	Can reformulate K-Means as an (online) Map-Reduce problem.
2014-08-29	http://arxiv.org/pdf/1101.2245v2.pdf	Invertible Bloom Lookup Tables	Computer Science	We can now enumerate the items in a Bloom filter!
