Date	URL	Title	Topic	Summary
2014-08-11	http://web.stanford.edu/~jpennin/papers/glove.pdf	GloVe: Global Vectors for Word Representation	Natural Language Processing	Semantic modelling: combine word-co-occurrence approach with word-context approach to get significantly better performance. Only consider non-zero elements of co-occurrence matrix in order to reduce noise. Not all training data is created equal: less in-domain data can outperform more out-of-domain data.
2014-08-13	http://research.microsoft.com/en-us/um/people/nickcr/wscd2014/papers/wscdchallenge2014dataiku.pdf	Dataiku’s Solution to Yandex’s Personalized Web Search Challenge	Data Science	Winning a Kaggle learning-to-rank challenge. Interesting parts: using collaborative filtering for search re-ranking, quality of search snippet as a feature, trying to break the class-labels in the dataset up further. Hyper-parameter tuning was important.
2014-08-15	http://arxiv.org/pdf/1207.4169.pdf	The Author-Topic Model for Authors and Documents	Natural Language Processing	Augmenting LDA topic modelling with author information: opens up applications in stylometry and recommendations.
2014-08-17	http://www.win.tue.nl/~laroyo/2L340/resources/Amazon-Recommendations.pdf	Item-to-Item Collaborative Filtering	Machine Learning	Collaborative filtering finds similar users (e.g. using cosine similarity) and recommends items that one likes but not the other. Slow: O(M+N) where M is the number of users and N is the number of items. Can speed it up using dimensionality reduction but that hurts performance. Cluster-based recommendations split the user population into categories and generate local recommendations within the categories. Faster than collaborative filtering but still slow. Usually poor performance because clustering is not granular enough. Search-based recommendations use past interests as search queries to generate recommendations. Performs poorly for customers with large purchase histories due to difficulty of selecting the relevant items to based the query on. Item-to-item collaborative filtering finds similar items to the ones that a user likes and recommends those (e.g. by looking at items that are frequently purchased together - very sparse so easy to compute, can be computed off-line; on-line complexity is only related to user purchase history so essentially constant time).
2014-08-17	http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf	Top 10 algorithms in data mining	Machine Learning	Gentle overview of some basic machine learning algorithms. Some interesting observations. K-means: can fit non-spherical clusters by using more sophisticated distance metrics (e.g. KL divergence) or we can run k-means with large k and then use agglomerative clustering as a post-processing step (this technique also means that we don't have to select k and aren't sensitive to initial centroid choice). Apriori algorithm for frequent-item-set inference: iteratively grow and prune item-sets to avoid combinatorial explosion. Ada boost algorithm: iteratively fit a weak algorithm to a data-set and re-weight the data by the error of the learner (i.e. in the next round we'll learn a model that prioritizes the errors of the previous one), then get overall prediction using weighted average of all learners: ensemble learning technique outperforms performance of the individual learners. K-NN: it's possible to remove most of the data-points without harming accuracy. Naive Bayes: get a non-linear decision boundary by including interaction terms between attributes.
2014-08-18	http://arxiv.org/pdf/1404.3056.pdf	Principles of Antifragile Software	Software Engineering	Antifragility: getting better in the presence of errors. Examples: code that patches itself when encountering a bug. E.g. Netflix randomly crashes servers or increases/decreases latency to test their distributed algorithms. Need to minimize impact of errors e.g. via TDD + continuous deployment.
