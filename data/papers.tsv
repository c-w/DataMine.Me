Date	URL	Title	Topic	Summary
2014-08-11	http://web.stanford.edu/~jpennin/papers/glove.pdf	GloVe: Global Vectors for Word Representation	Natural Language Processing	Semantic modelling: combine word-co-occurrence approach with word-context approach to get significantly better performance. Only consider non-zero elements of co-occurrence matrix in order to reduce noise. Not all training data is created equal: less in-domain data can outperform more out-of-domain data.
2014-08-13	http://research.microsoft.com/en-us/um/people/nickcr/wscd2014/papers/wscdchallenge2014dataiku.pdf	Dataiku’s Solution to Yandex’s Personalized Web Search Challenge	Data Science	Winning a Kaggle learning-to-rank challenge. Interesting parts: using collaborative filtering for search re-ranking, quality of search snippet as a feature, trying to break the class-labels in the dataset up further. Hyper-parameter tuning was important.
2014-08-15	http://arxiv.org/pdf/1207.4169.pdf	The Author-Topic Model for Authors and Documents	Natural Language Processing	Augmenting LDA topic modelling with author information: opens up applications in stylometry and recommendations.
2014-08-17	http://www.win.tue.nl/~laroyo/2L340/resources/Amazon-Recommendations.pdf	Item-to-Item Collaborative Filtering	Machine Learning	Collaborative filtering finds similar users (e.g. using cosine similarity) and recommends items that one likes but not the other. Slow: O(M+N) where M is the number of users and N is the number of items. Can speed it up using dimensionality reduction but that hurts performance. Cluster-based recommendations split the user population into categories and generate local recommendations within the categories. Faster than collaborative filtering but still slow. Usually poor performance because clustering is not granular enough. Search-based recommendations use past interests as search queries to generate recommendations. Performs poorly for customers with large purchase histories due to difficulty of selecting the relevant items to based the query on. Item-to-item collaborative filtering finds similar items to the ones that a user likes and recommends those (e.g. by looking at items that are frequently purchased together - very sparse so easy to compute, can be computed off-line; on-line complexity is only related to user purchase history so essentially constant time).
2014-08-17	http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf	Top 10 algorithms in data mining	Machine Learning	Gentle overview of some basic machine learning algorithms. Some interesting observations. K-means: can fit non-spherical clusters by using more sophisticated distance metrics (e.g. KL divergence) or we can run k-means with large k and then use agglomerative clustering as a post-processing step (this technique also means that we don't have to select k and aren't sensitive to initial centroid choice). Apriori algorithm for frequent-item-set inference: iteratively grow and prune item-sets to avoid combinatorial explosion. Ada boost algorithm: iteratively fit a weak algorithm to a data-set and re-weight the data by the error of the learner (i.e. in the next round we'll learn a model that prioritizes the errors of the previous one), then get overall prediction using weighted average of all learners: ensemble learning technique outperforms performance of the individual learners. K-NN: it's possible to remove most of the data-points without harming accuracy. Naive Bayes: get a non-linear decision boundary by including interaction terms between attributes.
2014-08-18	http://arxiv.org/pdf/1404.3056.pdf	Principles of Antifragile Software	Software Engineering	Antifragility: getting better in the presence of errors. Examples: code that patches itself when encountering a bug. E.g. Netflix randomly crashes servers or increases/decreases latency to test their distributed algorithms. Need to minimize impact of errors e.g. via TDD + continuous deployment.
2014-08-22	http://arxiv.org/pdf/1408.3218v1.pdf	Toward Automated Discovery of Artistic Influence	Computer Vision	Task: automatically finding all the influences on some painter. Unsupervised knowledge discovery task. Difficult. Solve correlated supervised task instead: style classification. Unsurprisingly, custom made high-level semantic features work better than generic low level features like SIFT.
2014-08-26	http://research.microsoft.com/pubs/167719/WhyFromNigeria.pdf	Why do Nigerian Scammers Say They are from Nigeria?	Machine Learning	Using reasoning based on ROC curves to find analytic bounds of classifier performance.
2014-08-26	http://delivery.acm.org/10.1145/1730000/1722966/p4-su.pdf	A Survey of Collaborative Filtering Techniques	Machine Learning	Main challenges in recommenders. Data sparsity: use dimensionality reduction (LSI, PCA) or try to do local recommendations in some cluster e.g. by using a decision tree or explicit clustering or external information (like product taxonomy). Scalability: there exists an incremental version of SVD,  if possible only compare co-rated items. Synonymy: LSI fixed this automatically or domain specific thesaurus. Shilling attacks: item-based more robust, explicitly account for it during data normalization, work with similarity residuals instead of absolute values. Memory-based collaborative filtering algorithm: find similar users (e.g. Pearson correlation or cosine similarity) and then aggregate their likes into recommendations (e.g. using voting or weighted averages). Model-based algorithm: Bayes net or clustering. Hybrids incorporating domain knowledge or extra structure (e.g. demographics) work best. Evaluation: MAE or RMSE or ROC AUC.
2014-08-26	http://wwwold.cs.umd.edu/~elaine/docs/scambaiter.pdf	Scambaiter: Understanding Targeted Nigerian Scams on Craigslist	Data Science	Setting up a honey-pot to collect data about 491 scams. IP address and reply-to email address that differs from sent-from address are highly indicative features of scams. Images in emails that include images hosted on external servers are uncannily effective at gathering personal information via emails.
2014-08-29	http://2013.berlinbuzzwords.de/sites/2013.berlinbuzzwords.de/files/slides/DanFilimon.pdf	Clustering data at scale	Machine Learning	Can reformulate K-Means as an (online) Map-Reduce problem.
2014-08-29	http://arxiv.org/pdf/1101.2245v2.pdf	Invertible Bloom Lookup Tables	Computer Science	We can now enumerate the items in a Bloom filter!
2014-12-07	http://storm.cis.fordham.edu/gweiss/papers/dmin07-weiss.pdf	Cost-Sensitive Learning vs. Sampling: Which is Best for Handling Unbalanced Classes with Unequal Error Costs?	Machine Learning	Two main approaches. 1. Use a cost sensitive algorithm i.e. encode a penalty for wrong classifications of a particular class in the objective function. 2. Use sampling to make the data-ratios more equal. Sampling is the way to go when there is no cost-sensitive version of the preferred learning algorithm or when we need to subsample the data anyway e.g. because we have too much of it. If we don't know the correct class weights, we should use area under ROC curve as objective function to find the correct sampling rate or class weights. For small data-sets, don't use under-sampling because we can't afford to reject training examples and cost-sensitive learning might be worse than over-sampling because the classifier doesn't have enough data-points to estimate the correct class weights. For large data-sets cost-sensitive learning is usually best.
2014-12-07	http://arxiv.org/pdf/1106.1813.pdf	SMOTE: Synthetic Minority Over-sampling Technique	Machine Learning	Better than over-sampling with replacement: create artificial examples by interpolating between k-nearest-neighbours. This leads to larger and less specific decision boundaries instead of over-sampling's more specific boundaries i.e. less chance of over-fitting. Extensions of the technique to non-numeric features exist.
2015-01-19	http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf	Machine Learning: The High-Interest Credit Card of Technical Debt	Software Engineering	Lessons from real-world machine learning at Google. The paper mentions several anti-patterns to avoid and how to address them. E.g. entanglement/tight coupling/changing anything changes everything. It's easy to build v1 of a machine learning system, very complicated to make changes without changing the behavior of the entire model. Mitigation: metrics for all the dimensions, ensembling, better regularization. E.g. hidden feedback loops. Behavior of system affects future training data. Mitigation: ... un-solved problem. E.g. unstable data dependencies. Changes in input data sources can affect behavior of model. Mitigation: data versioning, but this is expensive in terms of engineering. E.g. underutilized features. Features with small improvement in model performance carry high cost and risk (see unstable data dependencies). Mitigation: regularly evaluate features and prune mercilessly. E.g. correction cascades. Models that use other models as features lead to coupling and make it more difficult to improve the models because of complex effects on the down-stream models. Mitigation: add the extra features of the down-stream model directly to the upstream-model. E.g. glue code. Interfacing with machine learning systems lead to glue code to adapt to the interface. Usually this means tight-coupling with particular versions. Mitigation: re-write the algorithms in the system language. E.g. monitoring. Difficult to know the correct behavior of a constantly adapting system. Mitigation: assert that observed and predicted priors are similar and threshold model confidence before taking action.
2015-01-25	http://arxiv.org/pdf/1411.4555v1.pdf	Show and Tell: A Neural Image Caption Generator	Natural Language Processing	Google using deep neural nets to achieve infuriatingly good results in image description generation: 3x improvement in BLEU over the state of the art. What's the secret-sauce? They train a neural network on raw images and then feed that into a language generation neural network that predicts word N+1 in the description using the image representation generated by the image recognizer and words 1..N in the training descriptions (basically like decoding in machine translation).
