Date	URL	Title	Topic	Summary
2014-08-11	http://web.stanford.edu/~jpennin/papers/glove.pdf	GloVe: Global Vectors for Word Representation	Natural Language Processing	Semantic modelling: combine word-co-occurrence approach with word-context approach to get significantly better performance. Only consider non-zero elements of co-occurrence matrix in order to reduce noise. Not all training data is created equal: less in-domain data can outperform more out-of-domain data.
2014-08-13	http://research.microsoft.com/en-us/um/people/nickcr/wscd2014/papers/wscdchallenge2014dataiku.pdf	Dataiku’s Solution to Yandex’s Personalized Web Search Challenge	Data Science	Winning a Kaggle learning-to-rank challenge. Interesting parts: using collaborative filtering for search re-ranking, quality of search snippet as a feature, trying to break the class-labels in the dataset up further. Hyper-parameter tuning was important.
2014-08-15	http://arxiv.org/pdf/1207.4169.pdf	The Author-Topic Model for Authors and Documents	Natural Language Processing	Augmenting LDA topic modelling with author information: opens up applications in stylometry and recommendations.
2014-08-17	http://www.win.tue.nl/~laroyo/2L340/resources/Amazon-Recommendations.pdf	Item-to-Item Collaborative Filtering	Machine Learning	Collaborative filtering finds similar users (e.g. using cosine similarity) and recommends items that one likes but not the other. Slow: O(M+N) where M is the number of users and N is the number of items. Can speed it up using dimensionality reduction but that hurts performance. Cluster-based recommendations split the user population into categories and generate local recommendations within the categories. Faster than collaborative filtering but still slow. Usually poor performance because clustering is not granular enough. Search-based recommendations use past interests as search queries to generate recommendations. Performs poorly for customers with large purchase histories due to difficulty of selecting the relevant items to based the query on. Item-to-item collaborative filtering finds similar items to the ones that a user likes and recommends those (e.g. by looking at items that are frequently purchased together - very sparse so easy to compute, can be computed off-line; on-line complexity is only related to user purchase history so essentially constant time).
2014-08-17	http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf	Top 10 algorithms in data mining	Machine Learning	Gentle overview of some basic machine learning algorithms. Some interesting observations. K-means: can fit non-spherical clusters by using more sophisticated distance metrics (e.g. KL divergence) or we can run k-means with large k and then use agglomerative clustering as a post-processing step (this technique also means that we don't have to select k and aren't sensitive to initial centroid choice). Apriori algorithm for frequent-item-set inference: iteratively grow and prune item-sets to avoid combinatorial explosion. Ada boost algorithm: iteratively fit a weak algorithm to a data-set and re-weight the data by the error of the learner (i.e. in the next round we'll learn a model that prioritizes the errors of the previous one), then get overall prediction using weighted average of all learners: ensemble learning technique outperforms performance of the individual learners. K-NN: it's possible to remove most of the data-points without harming accuracy. Naive Bayes: get a non-linear decision boundary by including interaction terms between attributes.
2014-08-18	http://arxiv.org/pdf/1404.3056.pdf	Principles of Antifragile Software	Software Engineering	Antifragility: getting better in the presence of errors. Examples: code that patches itself when encountering a bug. E.g. Netflix randomly crashes servers or increases/decreases latency to test their distributed algorithms. Need to minimize impact of errors e.g. via TDD + continuous deployment.
2014-08-22	http://arxiv.org/pdf/1408.3218v1.pdf	Toward Automated Discovery of Artistic Influence	Computer Vision	Task: automatically finding all the influences on some painter. Unsupervised knowledge discovery task. Difficult. Solve correlated supervised task instead: style classification. Unsurprisingly, custom made high-level semantic features work better than generic low level features like SIFT.
2014-08-26	http://research.microsoft.com/pubs/167719/WhyFromNigeria.pdf	Why do Nigerian Scammers Say They are from Nigeria?	Machine Learning	Using reasoning based on ROC curves to find analytic bounds of classifier performance.
2014-08-26	http://delivery.acm.org/10.1145/1730000/1722966/p4-su.pdf	A Survey of Collaborative Filtering Techniques	Machine Learning	Main challenges in recommenders. Data sparsity: use dimensionality reduction (LSI, PCA) or try to do local recommendations in some cluster e.g. by using a decision tree or explicit clustering or external information (like product taxonomy). Scalability: there exists an incremental version of SVD,  if possible only compare co-rated items. Synonymy: LSI fixed this automatically or domain specific thesaurus. Shilling attacks: item-based more robust, explicitly account for it during data normalization, work with similarity residuals instead of absolute values. Memory-based collaborative filtering algorithm: find similar users (e.g. Pearson correlation or cosine similarity) and then aggregate their likes into recommendations (e.g. using voting or weighted averages). Model-based algorithm: Bayes net or clustering. Hybrids incorporating domain knowledge or extra structure (e.g. demographics) work best. Evaluation: MAE or RMSE or ROC AUC.
2014-08-26	http://wwwold.cs.umd.edu/~elaine/docs/scambaiter.pdf	Scambaiter: Understanding Targeted Nigerian Scams on Craigslist	Data Science	Setting up a honey-pot to collect data about 491 scams. IP address and reply-to email address that differs from sent-from address are highly indicative features of scams. Images in emails that include images hosted on external servers are uncannily effective at gathering personal information via emails.
2014-08-29	http://2013.berlinbuzzwords.de/sites/2013.berlinbuzzwords.de/files/slides/DanFilimon.pdf	Clustering data at scale	Machine Learning	Can reformulate K-Means as an (online) Map-Reduce problem.
2014-08-29	http://arxiv.org/pdf/1101.2245v2.pdf	Invertible Bloom Lookup Tables	Computer Science	We can now enumerate the items in a Bloom filter!
2014-12-07	http://storm.cis.fordham.edu/gweiss/papers/dmin07-weiss.pdf	Cost-Sensitive Learning vs. Sampling: Which is Best for Handling Unbalanced Classes with Unequal Error Costs?	Machine Learning	Two main approaches. 1. Use a cost sensitive algorithm i.e. encode a penalty for wrong classifications of a particular class in the objective function. 2. Use sampling to make the data-ratios more equal. Sampling is the way to go when there is no cost-sensitive version of the preferred learning algorithm or when we need to subsample the data anyway e.g. because we have too much of it. If we don't know the correct class weights, we should use area under ROC curve as objective function to find the correct sampling rate or class weights. For small data-sets, don't use under-sampling because we can't afford to reject training examples and cost-sensitive learning might be worse than over-sampling because the classifier doesn't have enough data-points to estimate the correct class weights. For large data-sets cost-sensitive learning is usually best.
2014-12-07	http://arxiv.org/pdf/1106.1813.pdf	SMOTE: Synthetic Minority Over-sampling Technique	Machine Learning	Better than over-sampling with replacement: create artificial examples by interpolating between k-nearest-neighbours. This leads to larger and less specific decision boundaries instead of over-sampling's more specific boundaries i.e. less chance of over-fitting. Extensions of the technique to non-numeric features exist.
2015-01-19	http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf	Machine Learning: The High-Interest Credit Card of Technical Debt	Software Engineering	Lessons from real-world machine learning at Google. The paper mentions several anti-patterns to avoid and how to address them. E.g. entanglement/tight coupling/changing anything changes everything. It's easy to build v1 of a machine learning system, very complicated to make changes without changing the behavior of the entire model. Mitigation: metrics for all the dimensions, ensembling, better regularization. E.g. hidden feedback loops. Behavior of system affects future training data. Mitigation: ... un-solved problem. E.g. unstable data dependencies. Changes in input data sources can affect behavior of model. Mitigation: data versioning, but this is expensive in terms of engineering. E.g. underutilized features. Features with small improvement in model performance carry high cost and risk (see unstable data dependencies). Mitigation: regularly evaluate features and prune mercilessly. E.g. correction cascades. Models that use other models as features lead to coupling and make it more difficult to improve the models because of complex effects on the down-stream models. Mitigation: add the extra features of the down-stream model directly to the upstream-model. E.g. glue code. Interfacing with machine learning systems lead to glue code to adapt to the interface. Usually this means tight-coupling with particular versions. Mitigation: re-write the algorithms in the system language. E.g. monitoring. Difficult to know the correct behavior of a constantly adapting system. Mitigation: assert that observed and predicted priors are similar and threshold model confidence before taking action.
2015-01-25	http://arxiv.org/pdf/1411.4555v1.pdf	Show and Tell: A Neural Image Caption Generator	Natural Language Processing	Google using deep neural nets to achieve infuriatingly good results in image description generation: 3x improvement in BLEU over the state of the art. What's the secret-sauce? They train a neural network on raw images and then feed that into a language generation neural network that predicts word N+1 in the description using the image representation generated by the image recognizer and words 1..N in the training descriptions (basically like decoding in machine translation).
2015-02-08	http://www.kurims.kyoto-u.ac.jp/EMIS/journals/RCE/V35/v35n2a03.pdf	Comparison between SVM and Logistic Regression: Which One is Better to Discriminate?	Machine Learning	Theoretical and experimental comparison of Support Vector Machines versus Logistic Regression. When the data is multi-variate, drawn from a mixture of distributions or strongly correlated SVM is better. SVM can also achieve better results with fewer features. Logistic Regression is better for some underlying distributions in the data (Poisson, Exponential, Normal).
2015-02-08	http://onlinelibrary.wiley.com/doi/10.1002/uog.2791/pdf	Support vector machines versus logistic regression: improving prospective performance in clinical decision-making	Machine Learning	Fairly one-sided comparison of logistic regression and support vector machines. The paper claims that the latter is less likely to over-fit, better handles outliers and can better handle non-linearities in the data via kernels. The paper backs the claims up by comparing the performances of the two classifiers on data-sets with less than 100 examples.
2015-02-18	http://www.uni-weimar.de/medien/webis/publications/papers/lipka_2010.pdf	Comparison of Language Identification Approaches on Short, Query-Style Texts	Natural Language Processing	Very short paper looking at different ways to determine the language of short fragments of text. They find that using an n-gram based classifier approach they can achieve almost 100% accuracy, beating vector space models and language models. This means that we probably have some highly discriminative n-grams between languages and a lot of non informative n-grams (consistent with what we'd expect intuitively). I was able to reproduce their result that using an n-gram based classifiers solves this problem almost perfectly.
2015-02-18	http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf	Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?	Machine Learning	The paper compares the performance of 179 implementations of assorted classifiers on 121 open data-sets without any data pre-processing or feature engineering. They find that random-forests and SVMs with non-linear kernels have the best performance on average in binary and multi-class settings.
2015-02-18	http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37195.pdf	Detecting Adversarial Advertisements in the Wild	Machine Learning	Interesting paper from Google on how much work goes into using relatively simple machine learning techniques (stochastic gradient descent) in an industry setting (learning at scale, high cost of mistakes, human-in-the-loop, changing data over time). The sections on monitoring and adapting their systems in production is especially interesting. When they re-train the production model, the new model must pass some precision/recall tests on held out data before it gets promoted to production. They also monitor the distribution of the input features and output decisions to assert that they stay reasonable similarly distributed/relevant over time.
2015-03-22	http://arxiv.org/pdf/1502.01710v1.pdf	Text Understanding from Scratch	Natural Language Processing	They train a convolutional neural network on raw text (input features are non-overlapping windows of characters). They create additional noisy training data by replacing words in their training sentences with synonyms taken out of a dictionary which helps in making the network more invariant under non-semantic changes (like adding noise in speech recognition or scaling/rotating input images in computer vision). They apply the model to a bunch of different task such as predicting the star-rating of an Amazon review based on sentiment, predicting the ontological category of a DBpedia article and categorizing news into topics. They get pretty good results, out-performing their baseline implementations of bag-of-words and word2vect. The paper is pretty cool because it demonstrates once again that we probably don't need to engineer features when using neural networks and large enough training data!
2015-03-22	http://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_visapp09.pdf	Fast Approximate Nearest Neighbors With Automatic Algorithm Configuration	Machine Learning	They released an algorithm that selects the fastest nearest-neighbor search algorithm for some data-set. For most data-sets (even in high dimensions), it turns out that searching trees of hierarchical k-means clusters or randomized kd-trees work well. It is not surprising but nevertheless interesting that decomposing a hard problem in high dimensions into many smaller problems in lower dimensions works well.
2015-03-22	http://vision.ece.ucsb.edu/~zuliani/Research/RANSAC/docs/RANSAC4Dummies.pdf	RANSAC for dummies	Statistics	Regression algorithm that can was designed to deal with lots of outliers; the algorithm can handle more than half of the data-set being outliers. First, we create random minimal sample sets from the data. The size of these sets should be just big enough to fit our regression e.g., two elements for a two-dimensional feature space. We then fit a regression to the minimal sample sets and check how many points in the full data-set are consistent with this regression up to some delta of error. Iterate a fixed number of times or until the consensus set is large enough. Problems: how do we determine the error threshold and minimal acceptable consensus set size?
2015-04-26	http://olivier.chapelle.cc/pub/tnn99.pdf	SVMs for Histogram-Based Image Classification	Computer Vision	The paper makes a case for using color histograms for image representation: invariant under scale, rotation, translation, easy to compute. They use linear SVMs and SVMs with fancy kernels on top of the color histogram representation. They find that exponentiating the histogram bins (x_i := x_i^a for 0 <= a <= 1) makes linear SVMs perform almost as well as fancy SVMs.
2015-04-27	http://ccr.sigcomm.org/online/files/p83-keshavA.pdf	How to Read a Paper	Meta	Classic short text presenting a three-step approach to reading academic papers efficiently. The first pass should be a very short weed-out pass: what are the conclusions, what is the impact, is it likely to be correct, is it relevant to your research. The second pass is there to find flaws in the argument (requires getting a good understanding of the main argument). If the paper is not 100% relevant to your research, stop now. Otherwise, the third pass is the deconstruction piece: what are the strong/weak points of the paper, reach the same conclusions as the paper given the same assumptions, what are good avenues for future work.
